<h1 align="center">leakyGPT üï∏Ô∏è</h1>
<p align="center">
<img src="banner.png" height="350px" width="500px">
</p>
<p align="center"><b>The one-stop extension to prevent any unintended secret exposures while interacting with chatGPT.</b></p>
<br><br>

## Description:
Since OpenAI does <a href="https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance">admits</a> that they can use our data to train their models, the looming threat of secret leakge as part of user prompts had to be addressed. While OpenAI does take steps to reduce the amount of personal information going into their training datasets, no individual or organization would want their secret keys or tokens to be a part of the training data. This is a major issue for companies whose teams largely depend on ChatGPT for their day-to-day work and could be passing sensitive data as part of their prompts.

Introducing leakyGPT, a browser extension that looks for any secret exposures (with the help of our <a href="./regexes.json">signatures</a>) within user prompts before they are submitted to chatGPT. The user can decide whether to prevent the prompt from being submitted or continue with it, thus helping prevent secrets from accidentally being trained in the datasets.

<h2> Let's Connect! </h2>
If you have any questions or feedback about Genzai or just want to connect with me, feel free to reach out via <a href="https://x.com/0x9747">Twitter/X</a>, <a href="https://in.linkedin.com/in/umair-nehri-49699317a">LinkedIn</a> or <a href="mailto:umairnehri9747@gmail.com">Email</a>.
</div>
